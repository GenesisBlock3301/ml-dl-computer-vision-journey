{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Deep Learning Concepts Explained (With TensorFlow Examples)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# ----------------------------\n",
    "# 1. A Single Neuron\n",
    "# ----------------------------\n",
    "# Theory: A single neuron computes a weighted sum + bias and applies an activation.\n",
    "# Use: Only for extremely simple problems.\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Dense(1, input_shape=(1,))  # 1 input, 1 output\n",
    "])\n",
    "\n",
    "# ----------------------------\n",
    "# 2. Fully Connected Neural Network\n",
    "# ----------------------------\n",
    "# Theory: Every input connects to every neuron in the next layer.\n",
    "# Use: For most dense/tabular data.\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Dense(16, activation='relu', input_shape=(10,)),\n",
    "    layers.Dense(8, activation='relu'),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "# ----------------------------\n",
    "# 3. Linear Unit as a Model\n",
    "# ----------------------------\n",
    "# Theory: No activation function. Useful for regression.\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Dense(1, input_shape=(1,), activation=None)  # Linear unit\n",
    "])\n",
    "\n",
    "# ----------------------------\n",
    "# 4. Multiple Input\n",
    "# ----------------------------\n",
    "# Theory: Input shape can be >1 (vector).\n",
    "# Use: When you have multiple features.\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Dense(4, input_shape=(3,))  # 3 input features\n",
    "])\n",
    "\n",
    "# ----------------------------\n",
    "# 5. Deep Neural Networks\n",
    "# ----------------------------\n",
    "# Theory: Many layers to capture more complex patterns.\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(100,)),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "# ----------------------------\n",
    "# 6. Layers\n",
    "# ----------------------------\n",
    "# Theory: A neural network is built with layers.\n",
    "# Types: Dense, Conv2D, LSTM, etc.\n",
    "\n",
    "# ----------------------------\n",
    "# 7. Activation Function\n",
    "# ----------------------------\n",
    "# Theory: Adds non-linearity\n",
    "# Use: relu for hidden layers, sigmoid for binary classification\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Dense(8, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# ----------------------------\n",
    "# 8. Stacking Dense Layers\n",
    "# ----------------------------\n",
    "# Theory: Layer-by-layer hierarchy for better learning.\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "# ----------------------------\n",
    "# 9. Loss Function\n",
    "# ----------------------------\n",
    "# Theory: Measures error between prediction and truth.\n",
    "# Use: 'mse' for regression, 'binary_crossentropy' for binary classification\n",
    "model.compile(loss='binary_crossentropy')\n",
    "\n",
    "# ----------------------------\n",
    "# 10. Optimizer\n",
    "# ----------------------------\n",
    "# Theory: Algorithm to update weights.\n",
    "# Use: SGD, Adam\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.01))\n",
    "\n",
    "# ----------------------------\n",
    "# 11. Learning Rate and Batch Size\n",
    "# ----------------------------\n",
    "# Learning rate: how big a step to take\n",
    "# Batch size: how many samples before updating weights\n",
    "# Use in model.fit():\n",
    "# model.fit(x, y, batch_size=32, epochs=10)\n",
    "\n",
    "# ----------------------------\n",
    "# 12. Overfitting & Underfitting\n",
    "# ----------------------------\n",
    "# Overfitting: high train accuracy, low val accuracy (memorizing)\n",
    "# Underfitting: both accuracies are low (not learning enough)\n",
    "# Solution: Add more data, use dropout, reduce model complexity\n",
    "\n",
    "# ----------------------------\n",
    "# 13. Early Stopping\n",
    "# ----------------------------\n",
    "# Theory: Stop training when validation loss doesn't improve\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "# ----------------------------\n",
    "# 14. Dropout\n",
    "# ----------------------------\n",
    "# Theory: Randomly disables some neurons during training to prevent overfitting\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "# ----------------------------\n",
    "# 15. Batch Normalization\n",
    "# ----------------------------\n",
    "# Theory: Normalize outputs of a layer. Helps training stability.\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Dense(64),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "# ----------------------------\n",
    "# 16. Classification Problem\n",
    "# ----------------------------\n",
    "# Theory: Predicting categories (binary or multiclass)\n",
    "# Use: sigmoid (binary), softmax (multiclass)\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Dense(10, activation='softmax')  # For 10-class classification\n",
    "])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
